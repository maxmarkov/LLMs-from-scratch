# Implement a GPT model from scratch ti generate text

## Coding an LLM architecture

The model architecture consists of token and positional embeddings, dropout, a series of transformer blocks, a final layer normalization, and a linear output layer. The final layer produces logits with the next word probabilities.

The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizerâ€™s vocabulary.