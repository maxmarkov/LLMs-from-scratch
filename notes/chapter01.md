# Chapter 01. Understanding LLMs

An LLM is a neural network designed to understand, generate, and respond to human-like text. They are termed “large” due to both the model’s size and the extensive datasets used for training.

Text generated by LLMs appears coherent and contextually relevant, but LLMs do not possess human-like consciousness. The vast quantities of text data they are trained on allow them to capture deep contextual information, demonstrating proficiency across a wide range of NLP tasks.

**Applications**:

- Chatbots and virtual assistants
- Knowledge retrieval
- Parsing and analysis
- Translation
- Summarization

**Points of attentions**:

- Data privacy: Sharing sensitive data with LLM providers.
- Device deployment: Challenges of deploying LLMs on customer devices.

## Step 1: foundation models (like GPT-3)

Pretrained networks with a few-shot capabilities. They can perform new tasks based on a few examples only. Trained to predict next word and can do text completion. Foundation models capture the inherent sequential nature of language. No need to manually label data since next word prediction is a form of self-labeling with assigning/extracting labels on the fly.

## Step 2: instruction fine-tuning

Instruction fine-tuning requires a labeled dataset.

## Transformer architecture

Modules: encoder and decoder

- **Encoder** processes the input text and generates embeddings representing its content.
- **Decoder** takes these embeddings and generates the output text.

The core element of transformer architechture is **self-attention**. Self-attention assigns weights to different words and tokens in a sequence, highlighting their relative importance to each other.

**BERT** (Bidirectional Encoder Representations from Transformers) specializes in masked word prediction and text classification tasks, focusing on understanding the context within text sequences.

**GPT** (Generative Pretrained Transformer) includes only the decoder portion of the transformer architecture and is used primarily for generative tasks like text completion. Decoder-style models, such as GPT, generate text by predicting one word at a time in an autoregressive manner, using previous outputs as inputs for future predictions. Interestingly, these models can perform translation tasks as well, a capability that emerges naturally from exposure to vast amounts of multilingual data in diverse contexts.

